---
title: "Deep learning"
author: "Raphael Mourad, Maître de Conférences, Université Paul Sabatier"
date: "10/02/2021"
output: html_document
---

## SETUP PROJECT FOLDER

Set up your project folder to your folder "atelier_INSERM".
```{r, setup}
knitr::opts_knit$set(root.dir = "/shared/ifbstor1/home/rmourad/Deep-Learning-for-Genomics-Training-main/")
knitr::opts_chunk$set(echo = TRUE)
```

## AIM OF THE SCRIPT

This script is used to train build and train deep learning models for binary classification of positive and negative peak sequences.

## LOAD LIBRARIES

Load R librairies.
Remove suppressPackageStartupMessages() for debugging!
```{r, libraries}
suppressPackageStartupMessages(library(GenomicRanges))
suppressPackageStartupMessages(library(Biostrings))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(tensorflow))
suppressPackageStartupMessages(library(keras))
suppressPackageStartupMessages(library(reticulate))
```

## LOAD FUNCTIONS

```{r, function}
suppressPackageStartupMessages(source("scriptR/functions.R"))
```

## SOME PARAMETERS

Define parameters for the analysis:  
- peakSize is the resized peak size. For machine/deep learning, it is easier to work with peaks have the same size.  
- kpeaks is the number of best peaks used. Note that all peaks can also be used.  
- expe is the ChIP-seq experiment. For instance, CTCF for CTCF ChIP-seq or POL2 for RNA Polymerase 2 ChIP-seq experiment. 
```{r, parameters}
peakSize=201
kpeaks=4000
expe="CTCF"
DNAletters=c("A","C","G","T")
vocab_size=length(DNAletters)
```


## INPUT FILE NAMES

Set file names, which includes:  
- positive peaks from ChIP-seq and negative peaks from the control generator function genNullSeqs().  
- bed (genomic ranges) and fasta files (corresponding DNA sequences).
```{r, files}
fileBedPos=paste0("data/bed/",expe,"_GM12878_hg19_",kpeaks,"_pos.bed")
fileBedNeg=paste0("data/bed/",expe,"_GM12878_hg19_",kpeaks,"_neg.bed")
fileFastaPos=paste0("data/fasta/",expe,"_GM12878_hg19_",kpeaks,"_pos.fa")
fileFastaNeg=paste0("data/fasta/",expe,"_GM12878_hg19_",kpeaks,"_neg.fa")
```

## LOAD PROCESSED DATA

Load ChIP-seq and control fasta sequences:
```{r, read}
peakPos.seq=readDNAStringSet(fileFastaPos)
peakNeg.seq=readDNAStringSet(fileFastaNeg)
print(peakPos.seq)
```

Bind positive and negative sequences, and make label:
```{r, bind}
peakAll.seq=c(peakPos.seq,peakNeg.seq)
label=c(rep(1,length(peakPos.seq)),rep(0,length(peakNeg.seq)))
```

Shuffle sequence indices:
```{r, shuffle}
idxS=sample(1:length(peakAll.seq))
peakAllS.seq=peakAll.seq[idxS]
labelS=label[idxS]
print(labelS[1:5])
```

Split train and test indices:
```{r, split}
percTrain=0.7
idxTrain=1:(ceiling(length(labelS)*percTrain))
idxTest=(length(idxTrain)+1):length(labelS)
labelTrain=labelS[idxTrain]
labelTest=labelS[idxTest]
```

## DEEP LEARNING METHODS


One hot encoding
```{r, onehot}
oneHotTrain=convertOneHot(peakAllS.seq[idxTrain,])
oneHotTest=convertOneHot(peakAllS.seq[idxTest,])
```

CNN 1 (simple convolution + 10 dense). 
Simple yet very efficient model.
```{r, cnn1}
kernelSize=16
model <- keras_model_sequential()
model %>% 
  layer_conv_1d(filters = 128, kernel_size = kernelSize, activation = 'relu',
                input_shape = c(peakSize,vocab_size), name="conv1d_cnn1") %>% 
  layer_global_max_pooling_1d() %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 10, activation = "relu", name="10dense_cnn1") %>%
  layer_dense(units = 1, activation = 'linear', name="2ndlastlayer_cnn1") %>%
  layer_dense(units = 1, activation = 'sigmoid', name="lastlayer_cnn1") 
```

CNN 2 (parallel convolution). 
Useful to model different DNA motif sizes.
```{r, cnn2}
if(T){
inputs <- layer_input(shape = c(peakSize,vocab_size)) 
conv1 <- inputs %>%
  layer_conv_1d(filters = 256, kernel_size = 8, activation = 'relu') %>% 
  layer_global_max_pooling_1d() %>% 
  layer_dropout(rate = 0.5) 
conv2 <- inputs %>%
  layer_conv_1d(filters = 256, kernel_size = 16, activation = 'relu') %>% 
  layer_global_max_pooling_1d() %>% 
  layer_dropout(rate = 0.5) 
conv3 <- inputs %>%
  layer_conv_1d(filters = 256, kernel_size = 24, activation = 'relu') %>% 
  layer_global_max_pooling_1d() %>% 
  layer_dropout(rate = 0.5) 
outputs <- layer_concatenate(c(conv1,conv2,conv3)) %>%
  layer_dense(units = 10, activation = "relu") %>%
  layer_dense(units = 1, activation = 'sigmoid') 
model <- keras_model(inputs = inputs, outputs = outputs)
}
```

CNN 3 (double convolution + 10 dense). 
Useful for computer vision, but not for DNA sequences.
```{r, cnn3}
if(F){
model <- keras_model_sequential()
model %>% 
  layer_conv_1d(filters = 256, kernel_size = 8, activation = 'relu',
                input_shape = c(peakSize-1,vocab_size)) %>% 
  layer_conv_1d(filters = 256, kernel_size = 3, activation = 'relu',
                input_shape = c(peakSize-1,vocab_size)) %>% 
  layer_global_max_pooling_1d() %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 10, activation = "relu") %>%
  layer_dense(units = 1, activation = 'sigmoid') 
}
```

RNN with LSTM (no CNN). Bad predictions.
Bad results. We need to include a convolution layer before the lstm layer!
Slow (not made for efficient parallel computing)
```{r, lstm}
if(T){
model <- keras_model_sequential()
model %>% 
  layer_lstm(128,input_shape = c(peakSize-1,vocab_size)) %>%
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 10, activation = "relu") %>%
  layer_dense(1) %>%
  layer_activation("sigmoid")
}
```

CNN + LTSM. 
Simple yet very efficient model.
```{r, cnn_lstm}
if(T){
model <- keras_model_sequential()
model %>% 
  layer_conv_1d(filters = 128, kernel_size = 16, activation = 'relu',
                input_shape = c(peakSize-1,vocab_size)) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_lstm(12) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 20, activation = "relu") %>%
  layer_dense(units = 1, activation = 'sigmoid') 
}
```


Model summary
```{r, summary}
model %>% summary()
```

Compile the model
SGD and Adagrad = bad results
Adam and RMSprop = good results
```{r, compile}
model %>% compile(
  optimizer = 'rmsprop', # adam or rmsprop are great here
  loss = 'binary_crossentropy',
  metrics = list('accuracy')
)
```

Train the model
```{r, train}
history <- model %>% fit(
  oneHotTrain,
  labelTrain,
  epochs = 10, 
  batch_size = 128,
  validation_split = 0.2,
  verbose=1
)
plot(history)
```

Prediction
```{r, predict}
predCNN=predict(model,oneHotTest)
rocCNN_motif=pROC::roc(as.factor(labelTest),predCNN,ci=T)
plot(rocCNN_motif,main=paste0("AUROC: ", round(pROC::auc(rocCNN_motif),3)))
```

Accuracy and loss
```{r, acc}
acc_loss <- model %>% evaluate(oneHotTest, labelTest)
acc_loss
```

Save model
```{r, save_model}
file_model=paste0("results/model/CNN1_model_",expe,".hdf5")
save_model_hdf5(model,file_model)
```




